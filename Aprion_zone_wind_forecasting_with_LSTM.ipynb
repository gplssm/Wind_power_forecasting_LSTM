{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0502e45b",
   "metadata": {},
   "source": [
    "# Wind power forecasting in the Amprion TSO zone\n",
    "\n",
    "This is a capstone project from the course series [Advanced Data Science Specialization](https://www.coursera.org/specializations/advanced-data-science-ibm). It is a small example for show-casing what was learned in the course.\n",
    "\n",
    "- Goal: Forecast next quarter-hourly value based on the past 24 values\n",
    "- Data source: [Actual wind generation](https://www.amprion.net/Netzkennzahlen/Windenergieeinspeisung/) in the TSO zone\n",
    "- Feature engineering: prepare historic data and datetime information\n",
    "- Model: Single-layer LSTM model\n",
    "\n",
    "**Usage**\n",
    "\n",
    "You might have to install the requirements from here: [https://raw.githubusercontent.com/gplssm/Wind_power_forecasting_LSTM/main/requirements.txt](https://raw.githubusercontent.com/gplssm/Wind_power_forecasting_LSTM/main/requirements.txt)\n",
    "\n",
    "Use the first 3 code cells for parametrizing data preparation and model definition and training.\n",
    "For example, switch `train_model` to False for loading an already trained model from file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d0a31e",
   "metadata": {},
   "source": [
    "**Define data range for testing and training**\n",
    "\n",
    "Start and end day are both included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ad860",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_day = \"2019-01-01\"\n",
    "end_day = \"2020-12-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e983996",
   "metadata": {},
   "source": [
    "**Define parameters for data pre-processing and feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41af29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_past = 24\n",
    "timesteps_future = 1\n",
    "stride = 1\n",
    "batch_size = 64\n",
    "test_fraction = 0.2 # possible range 0..1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f65d88",
   "metadata": {},
   "source": [
    "**Parameters for model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a42ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = True\n",
    "stateful = False\n",
    "lstm_nodes = 144\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_model = f\"wind_forecasting_{'stateful' if stateful else 'stateless'}_LSTM-{lstm_nodes}-nodes_{epochs}-epochs_adam_mae.keras\"\n",
    "print(filename_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13787b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbf4bc0",
   "metadata": {},
   "source": [
    "## Retrieve raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d05f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amprion_wind_data(start_day, end_day):\n",
    "    url = f\"https://www.amprion.net/api/grid-data/items/csv/WINDDATEN/{start_day}/{end_day}\"\n",
    "    filename = f\"wind_amprion_from_{start_day}_to_{end_day}.csv\"\n",
    "    \n",
    "    if not os.path.isfile(filename):\n",
    "        urlretrieve(url, filename)\n",
    "    df = pd.read_csv(\n",
    "        filename,\n",
    "        delimiter=\";\",\n",
    "        decimal=\",\",\n",
    "        dtype={\n",
    "            \"Online Hochrechnung [MW]\": float,\n",
    "            \"8:00 Uhr Prognose [MW]\": float\n",
    "        })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a90aa6",
   "metadata": {},
   "source": [
    "Retrieve raw data from Aprion Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf0693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_amprion_wind_data(start_day, end_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa0333",
   "metadata": {},
   "source": [
    "Combine date and time information into a single column a set this as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"starttime\"] = data[\"Uhrzeit\"].str.split(\" - \", expand=True)[0]\n",
    "data[\"full_date\"] = data[[\"Datum\", \"starttime\"]].agg(\" \".join, axis=1)\n",
    "data[\"date\"] = pd.to_datetime(data[\"full_date\"], format=\"%d.%m.%Y %H:%M\")\n",
    "data.drop([\"Datum\", \"Uhrzeit\", \"starttime\", \"full_date\"], axis=1, inplace=True)\n",
    "data.set_index(\"date\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84320b",
   "metadata": {},
   "source": [
    "Check the column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b90e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f9174",
   "metadata": {},
   "source": [
    "Are NaNs included in the data columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e8abb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ec05a",
   "metadata": {},
   "source": [
    "Are there missing dates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7743873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_date_range = pd.date_range(data.index.min(), data.index.max(), freq=\"15Min\")\n",
    "ref_date_range[~ref_date_range.isin(data.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160cfb94",
   "metadata": {},
   "source": [
    "Are the duplicate dates in the index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d27af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.index.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49cf87",
   "metadata": {},
   "source": [
    "Yepp! Better remove them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d712d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data.index.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11089110",
   "metadata": {},
   "source": [
    "## Fill missing data by linear interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858184d",
   "metadata": {},
   "source": [
    "Missing index values are inserted first with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab966e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reindex(ref_date_range, fill_value=None)\n",
    "data[data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a7c8b",
   "metadata": {},
   "source": [
    "Prior existing NaNs and newly inserted NaNs are replaced by linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.interpolate(axis=0)\n",
    "data[data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd7428",
   "metadata": {},
   "source": [
    "## Get a visual impression of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    data, \n",
    "    x=data.index, \n",
    "    #y=[\"Online Hochrechnung [MW]\", \"8:00 Uhr Prognose [MW]\"],\n",
    "    y=[\"Online Hochrechnung [MW]\"],\n",
    "    width=960,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.update_layout(showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fce96",
   "metadata": {},
   "source": [
    "### Distribution of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(data, x=\"Online Hochrechnung [MW]\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a35b1ec",
   "metadata": {},
   "source": [
    "### Indentify seasonality and repeating patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations = [\"Y\", \"M\", \"W\", \"D\"]\n",
    "fig = make_subplots(rows=len(aggregations), cols=1)\n",
    "\n",
    "for row, resampler in enumerate(aggregations, start=1):\n",
    "    resampled = data[\"Online Hochrechnung [MW]\"].resample(resampler).mean()\n",
    "\n",
    "    fig.append_trace(\n",
    "        go.Scatter(\n",
    "            x=resampled.index,\n",
    "            y=resampled,\n",
    "        ), row=row, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80fcd9f",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "- A trend of increasing electricity generation by wind power\n",
    "- Seasonality: during summer wind power generation is lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e1f726",
   "metadata": {},
   "source": [
    "### Do certain months have more variation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variations = data.copy()\n",
    "variations[\"week\"] = variations.index.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc64575",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "sns.violinplot(x=variations[\"week\"], y=variations[\"Online Hochrechnung [MW]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04f4ea",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "- Highest wind generation during winter months\n",
    "- Less variation during summer\n",
    "- Smaller maximum power generation during summer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ca5ae",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa7974",
   "metadata": {},
   "source": [
    "## Transform time stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69045092",
   "metadata": {},
   "source": [
    "Time stamp data is transformed to separate columns that i.e. represent month of the year to reflect seasonality. This step is performed for easier interpretation of data during model training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dba9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"year\"] = data.index.year\n",
    "data[\"month\"] = data.index.month\n",
    "data[\"week\"] = data.index.isocalendar().week\n",
    "data[\"day\"] = data.index.dayofyear\n",
    "data[\"hour\"] = data.index.hour\n",
    "data[\"quarter_hour\"] = (data.index.minute.to_numpy() / 15).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8888df4",
   "metadata": {},
   "source": [
    "## Reshape data and and prepare for Keras API\n",
    "\n",
    "- **Timesteps**: define forecast length in number of timesteps. Typically, a symmetric time window is used to predict _n_ future values by data from _n_ timesteps in the past. Here, we use 144 ($=36 h \\cdot 4$) in order to predict wind energy production at 12 am for the entire next day.\n",
    "- **Stride**: is the number of timesteps the windows moves. Two options:\n",
    "\n",
    "  1. As we want to predict data for the entire next day each day a 12 o'clock, we choose 96 ($=24 h \\cdot 4$) as stride in order to make the prediction once a day\n",
    "  2. As this is only relevant for training, we should use a stride of 1 in order to consider as much data as possbile during training\n",
    "\n",
    "\n",
    "- **Batch size** describes how many samples are shown at once during training of the network. 64 is a typical value that's good to start with\n",
    "- **Training set size**  is the size of the entire dataset that is used for training. Often, a split of $90\\,\\%$ training data and $10\\,\\%$ test data is used. It is important that the training set size is a multiple of batch size ($training\\_set\\_size \\mod batch\\_size \\overset{!}{=} 0$)\n",
    "- **Input shape:** is defined by the Keras API. It expects $input\\_shape = (batches \\times timesteps \\times features)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110cff06",
   "metadata": {},
   "source": [
    "### Extract features and normalize data\n",
    "\n",
    "Use MinMaxScaler from sklearn and explain why this one should be used here\n",
    "\n",
    "**Hint** (for later research):\n",
    "It might be useful to apply a scaler to hour, month, ... that is able to reflect cyclic characteristic of these features. Read more in [this article](http://blog.davidkaleko.com/feature-engineering-cyclical-features.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a060b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['Online Hochrechnung [MW]', 'year', 'month', 'week', 'day', 'hour', 'quarter_hour']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pickle import dump\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features = scaler.fit_transform(features)\n",
    "features\n",
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf46a4",
   "metadata": {},
   "source": [
    "### Training data, validation data and batch size\n",
    "Split data into training and validation dataset considering the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46f53e",
   "metadata": {},
   "source": [
    "Identify exact length of training data matrix considering the constraint that training set length must be multiple of batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c25efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_set_length(data_length, test_fraction, batch_size):\n",
    "    \n",
    "    # Incrementaly increase training set length by 1\n",
    "    for increment in range(0, batch_size - 1):\n",
    "        train_length = round(data_length * (1 - test_fraction)) + increment\n",
    "        \n",
    "        # Return training set length if training set length is a multiple of batch_size\n",
    "        if ((train_length - (timesteps_past - 1))%(batch_size)) == 0:\n",
    "            return train_length   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = get_training_set_length(features.shape[0], test_fraction, batch_size)\n",
    "print(train_length)\n",
    "print(train_length/(timesteps_past + (stride - 1) * batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8781e0",
   "metadata": {},
   "source": [
    "Extract training and test data from the entire dataset. Beware, test data is simply taken from the end of the entire sequence. This might bias the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b2f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features[:train_length]\n",
    "features_test = features[train_length + 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac84654",
   "metadata": {},
   "source": [
    "### Transform data according to prediction window\n",
    "\n",
    "Labels (produced wind energy) are extracted from features and shifted by _timesteps_future_ in order to make the model learn to predict values from past observed values.\n",
    "\n",
    "There are two options to correlate time features with wind energy production\n",
    "\n",
    "1. Time information is related to x **(chosen)**\n",
    "2. Time information is related to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_start = timesteps_past + timesteps_future\n",
    "labels_end = labels_start + train_length\n",
    "\n",
    "x_train = features[:train_length]\n",
    "y_train = features[labels_start:labels_end, 0]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e3342",
   "metadata": {},
   "source": [
    "Transform also test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361fca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_length_x = len(features[train_length:(-timesteps_past - timesteps_future)])\n",
    "for decrement in range(0, batch_size - 1):\n",
    "    test_length_x = total_test_length_x - decrement\n",
    "    \n",
    "    if (test_length_x - (timesteps_past - 1))%batch_size == 0:\n",
    "        break\n",
    "#(test_length_x-23)/64\n",
    "test_length_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_end = train_length + test_length_x\n",
    "test_labels_start = train_length + timesteps_past + timesteps_future\n",
    "test_labels_end = train_length + labels_start + test_length_x\n",
    "\n",
    "x_test = features[train_length:x_test_end]\n",
    "y_test = features[test_labels_start:test_labels_end, 0]\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1cb1cd",
   "metadata": {},
   "source": [
    "## Performance metric and optimizer choice\n",
    "\n",
    "[This article](https://otexts.com/fpp2/accuracy.html#scale-dependent-errors) gives a good overview on performance metrics for time series forecasting. Here, we choose mean absolute error (MAE) as perfomance metric (loss function) because\n",
    "\n",
    "- It's simple to explain\n",
    "- As data is scaled to the range 0..1, the unit doesn't matter\n",
    "\n",
    "As optimizer to find best weights _Adam_ is used. It's popular optimizer and often used with LSTM for time series forecasting.\n",
    "\n",
    "For performance evualation, mean absolute percentage error (MAPE) is calculated too, because it's easier to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43efce2",
   "metadata": {},
   "source": [
    "Bring features and labels into shape of $(samples \\times timesteps \\times features)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    sequence_length=timesteps_past,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "dataset_test = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    sequence_length=timesteps_past,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "for batch in dataset_train:\n",
    "    inputs, targets = batch\n",
    "    \n",
    "    inputs_shape = inputs.numpy().shape\n",
    "    assert inputs_shape[0] == batch_size\n",
    "    assert inputs_shape[1] == timesteps_past\n",
    "    assert inputs_shape[2] == 7\n",
    "    assert targets.numpy().shape[0] == batch_size\n",
    "\n",
    "print(inputs_shape, targets.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset_test: \n",
    "    inputs, targets = batch\n",
    "    \n",
    "    inputs_shape = inputs.numpy().shape\n",
    "    assert inputs_shape[0] == batch_size\n",
    "    assert inputs_shape[1] == timesteps_past\n",
    "    assert inputs_shape[2] == 7\n",
    "    assert targets.numpy().shape[0] == batch_size\n",
    "print(inputs_shape, targets.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d2a50",
   "metadata": {},
   "source": [
    "## Defining the neurol network architecture\n",
    "\n",
    "- Input shape: only timesteps and number features are defined\n",
    "- Single-layer LSTM network with 144 units\n",
    "- Dense layer with one neuron representing one feature to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909bc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "input_layer = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]), batch_size=batch_size)\n",
    "lstm_2 = keras.layers.LSTM(lstm_nodes, stateful=stateful, return_sequences=False)(input_layer)\n",
    "output_layer = keras.layers.Dense(1)(lstm_2)\n",
    "\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "    loss=[\"mae\"],\n",
    "    metrics=[\n",
    "        keras.metrics.mean_squared_error,\n",
    "        keras.metrics.mean_absolute_error, #already represented by loss\n",
    "        keras.metrics.mean_absolute_percentage_error],\n",
    "    run_eagerly=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aec770",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "\n",
    "\n",
    "**Hint:** In the Keras [timeseries forecasting example](https://keras.io/examples/timeseries/timeseries_weather_forecasting/#training) model checkpoints and early stopping, based on validation loss is used. Consider to follow this approach for faster training, once validation data is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971127ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    history = model.fit(\n",
    "        dataset_train,\n",
    "        epochs=epochs,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    model.save(filename_model)\n",
    "    print(history.history)\n",
    "else:\n",
    "    model = keras.models.load_model(filename_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2891bd1f",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb6ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3e90e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for num, (x, y) in enumerate(dataset_test): \n",
    "    \n",
    "    true_labels.extend(y.numpy())\n",
    "\n",
    "    prediction = model.predict(x)\n",
    "    predictions.extend(prediction)\n",
    "\n",
    "predictions = np.array(predictions).flatten()\n",
    "prediction_evaluation = pd.DataFrame(\n",
    "    {\n",
    "        \"predictions\": predictions,\n",
    "        \"labels\": true_labels\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e7d38",
   "metadata": {},
   "source": [
    "### Scale data back to original unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a235d",
   "metadata": {},
   "source": [
    "scale back to original data range of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(series, n_features):\n",
    "\n",
    "    trainPredict_dataset_like = np.zeros(shape=(len(series), n_features))\n",
    "    trainPredict_dataset_like[:, 0] = series.to_numpy()\n",
    "    return scaler.inverse_transform(trainPredict_dataset_like)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd184652",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_evaluation[\"labels_MWh\"] = inverse_transform(prediction_evaluation[\"labels\"], 7)\n",
    "prediction_evaluation[\"predictions_MWh\"] = inverse_transform(prediction_evaluation[\"predictions\"], 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4bb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    prediction_evaluation, \n",
    "    x=range(len(predictions)), \n",
    "    y=[\"labels_MWh\", \"predictions_MWh\"],\n",
    "    width=960,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.update_layout(showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ddd4c",
   "metadata": {},
   "source": [
    "### Calculate performance indicators\n",
    "\n",
    "Re-calculate performance indicators manually\n",
    "\n",
    "- **Mean absolute error** as: $MAE=\\frac{1}{n} \\cdot \\sum_{t=1}^{n} \\left| y_{t}-\\hat{y}_t \\right|$\n",
    "- **Mean absolute percentage error** as: $MAPE=\\frac{100}{n} \\cdot \\sum_{t=1}^{n} \\left| \\frac{y_{t}-\\hat{y}_t}{y_{t}} \\right|$\n",
    "\n",
    "But first, per row, hence, for each time step in order to explore the distribution of deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bc19a",
   "metadata": {},
   "source": [
    "Seems like to match quite well, but it might be not matching in time! Predictions seem to be lagging and like predicting the value of the last timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d4642",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_evaluation[\"difference_absolute\"] = (prediction_evaluation[\"labels_MWh\"] - prediction_evaluation[\"predictions_MWh\"]).abs()\n",
    "prediction_evaluation[\"difference_percentage\"] = (prediction_evaluation[\"difference_absolute\"] / prediction_evaluation[\"labels_MWh\"]) * 100\n",
    "prediction_evaluation.loc[prediction_evaluation[\"difference_percentage\"] == np.inf, \"difference_percentage\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be54d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(prediction_evaluation, x=[\"difference_percentage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean absolute error (MAE): {prediction_evaluation['difference_absolute'].mean()}\")\n",
    "print(f\"Mean absolute percentage error (MAPE): {prediction_evaluation['difference_percentage'].mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
